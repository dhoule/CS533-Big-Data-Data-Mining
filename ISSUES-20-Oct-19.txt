Ashwith and I have split the files to go over them. I am looking at the clusters, dbscan, and mpi_main files, while Ashwith is looking at the other files. So far, I noticed several things; some are annoyances, some are actual issues. Speaking with Ashwith earlier this afternoon, he has also noticed some of these issues, that need to be worked through. 

One thing I have noticed while just glancing over the code, is it is written by several different people, on different IDEs. This doesn't hurt performance of the code, but it does hamper readability. I have been spending time reorganizing the code, as I go over it, to make it easier to read; this includes fixing the spacing, as the other programmers had wildly different tab sizes.

The second thing that is a problem, is the commenting is few and far between. This does not help with maintainability, or scalability for Dr. Che's project right now. It is also making reading the code difficult, as Ashwith and I are also learning MPI programming while doing this. We are commenting as we go over the code. Especially when we come to a library function call that hasn't been seen before. Dr. Sinha and I spoke to Michael on Monday. There are a total of 14 cores on the rocks cluster. The MPI library being used if: "Open MPI: 2.1.1". I determined this by running `ompi_info` on the cluster. The API documentation I have been using is located at: https://www.open-mpi.org/doc/v2.1/. 

Another thing I have seen, is that they use their own shorthand when naming their variables. I don't mean the iterables, like i,j,k, etc. I mean the class variables that are in the global scope of each node. Trying to determine a variableâ€™s purpose from its name, and sometimes its usage, has not been going well. Rereading the specified section of the paper might help get some context, because that would help to rename them to something more intuitive. 

There are also logic blocks that I am unable to determine what they do. These blocks are sometimes 2-4 loops inside each other. I even saw one of these blocks that looked like it has a possibility for buffer overflow.

That brings me to this point. They are using non-blocking MPI calls; MPI_I***() functions; without any MPI_Wait() calls. After speaking with Indranil Roy, he stated that you typically use the non-blocking calls when you are absolutely sure of the size of the message. This is so the node can continue to work on other things. The input file is tested to make sure it matches some constraints. I still feel like they are playing it fast and loose, but that could be from my ignorance of MPI programming. Determining what exactly is being sent/received is going to require numerous cout() statements. This is going to be a pain, as this is asynchronous programming, where the output system interrupts are not atomic.

Onto the input file. The two sample data files given by NWU, are not setup the same. The binary file requires the number of points, and the number of dimensions, to be at the beginning of the file. All of the numbers after that, must be equal to the multiple of those two numbers; makes sense. I have tried a couple different setups when creating a binary file, and they all fail. I have another idea of how the file is setup, but I need to do some more investigation of how the sample file is truly read. I am using the following SO answer to build the binary file: https://stackoverflow.com/questions/28242813/how-to-convert-a-text-file-to-binary-file-using-linux-commands

I believe that if Ashwith and I have a smaller, known dataset, that we have access to the non-binary version of, it will make it easier for troubleshooting. We will be able to check the output of the DDBScan against the output of the PDBScan. By "smaller," I mean less data points with only 3 attributes. This should make the results quicker, and easier to represent, say in a graph. Also, only running with 2 nodes will make the output easier to interpret, in the beginning, to fix/finish the code.

Another thing I have noticed is the design of the nodes in the communication environment. This is not a "master-slave"/"server-client" setup. It's a peer-to-peer; but with one of importance; setup. The only thing that node 0 does differently is call the cout() function; that I have seen so far. Node 0 has been defined as the `proc_of_interest` variable. That might be why the number of nodes/processes must be divisible by 2.