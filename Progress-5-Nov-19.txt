Hello,
Aswith Danda and I are working on the distributed version of the SnG Clustering algorithm for DBScan. We are using the the CS Rock cluster. According to Michael Barkdoll, the CS cluster has 14 cores available. Beyond that, I have not asked for specifics, yet. The program is written in C++, and uses the openMPI and PnetCDF libraries. 

The program only takes binary files right now. The number of nodes in the system must also be a factor of 2; 2^n, 2,4,8,16, etc. All output files are in netCDF format. This file format can be read by MatLab, which means python can do it too, for free. That is why I sent the python script I created to everyone. The comments must be read before using it though, as it's a rough script built to do a single job. 

During the meeting Ashwith and I had on Friday, 1 Nov 19, he had the good idea to run the code with only 2 clusters in the system; I had only been running it with 8, as stated in the README. When checking the results, the output was correct. Given this new discovery, I ran several more tests, using two different datasets. The results are as follows:

The given dataset; 50,000 points, 10 dimensions:
cores, run, epsilon, minpts, stated_clusters, found_clusters
2    ,   1,      25,     5,              51,             51
2    ,   2,      25,     5,              51,             51
2    ,   3,      25,     5,              51,             51
2    ,   4,      25,     5,              51,             51
2    ,   5,      25,     5,              51,             51
4    ,   1,      25,     5,              51,             49
4    ,   2,      25,     5,              51,             49
4    ,   3,      25,     5,              51,             49
4    ,   4,      25,     5,              51,             49
4    ,   5,      25,     5,              51,             49
8    ,   1,      25,     5,              51,             45
8    ,   2,      25,     5,              51,             45
8    ,   3,      25,     5,              51,             45
8    ,   4,      25,     5,              51,             45
8    ,   5,      25,     5,              51,             45
16   ,   1,      25,     5,              51,             42
16   ,   2,      25,     5,              51,             42
16   ,   3,      25,     5,              51,             42
16   ,   4,      25,     5,              51,             42
16   ,   5,      25,     5,              51,             42

A custom binary file dataset(made from NWUs available datasets); 61,440 points, 3 dimensions:
cores, run, epsilon, minpts, stated_clusters, found_clusters
2    ,   1,    0.02,     5,               6,              6
2    ,   2,    0.02,     5,               6,              6
2    ,   3,    0.02,     5,               6,              6
2    ,   4,    0.02,     5,               6,              6
2    ,   5,    0.02,     5,               6,              6
4    ,   1,    0.02,     5,               6,              5
4    ,   2,    0.02,     5,               6,              5
4    ,   3,    0.02,     5,               6,              5
4    ,   4,    0.02,     5,               6,              5
4    ,   5,    0.02,     5,               6,              5
8    ,   1,    0.02,     5,               6,              5
8    ,   2,    0.02,     5,               6,              5
8    ,   3,    0.02,     5,               6,              5
8    ,   4,    0.02,     5,               6,              5
8    ,   5,    0.02,     5,               6,              5
16   ,   1,    0.02,     5,               6,              5
16   ,   2,    0.02,     5,               6,              5
16   ,   3,    0.02,     5,               6,              5
16   ,   4,    0.02,     5,               6,              5
16   ,   5,    0.02,     5,               6,              5

As you can see, as you increase the number of nodes, the integrity of the output data decreases. The `stated_clusters` column is the value given by the program. The `found_clusters` column, is from inspecting the output file. Ashwith and I have split the files in half. I'm looking at the cluster, dbscan, and mpi_main files. Aswith is looking into the others.

Ashwith is the only one to have seen the results of the tests above. I asked Joshua to run the same type of tests on the parallel version of the code. I didn't tell him the results of my test, to hopefully not hamper any results.

I believe I have narrowed the problem down to 2 functions in the dbscan.cpp file. They are both large functions, though, so it is going to take a while. The code has almost no comments, and when there is a comment, it's not very helpful. The naming of the variables is also not very intuitive, so determining their purpose is a bit of a chore. 

Ashwith and I are both learning MPI programming as we go through this project. I have a bit of an advantage right now, as I'm currently in the Distributed Systems class. MPI programming is more difficult than asynchronous programming. Operations, like output system interrupts, that are usually atomic, are not. Output debugging can become erratic, in the sense that the output from one node, can end up in the middle of the output from another node.

Our continual plan, is to keep learning the code, adding comments as we go. I'm going to focus my efforts on those 2 functions mentioned earlier. My tests have shown that every node knows the total number of clusters, as well as other info about the system. This is going to be very tedious work. The values of variables are going to have to be checked before and after each logic block of code. This will help determine the purpose of each block. I believe, right now, the problem is a certain math error when assigning cluster IDs.

